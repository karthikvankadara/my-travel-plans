# Project-1: Data Modelling with Postgres

## Overview

This is a Udacity [Data Engineering](https://www.udacity.com/course/data-engineer-nanodegree--nd027 "Data Engineering") project for handling the data generated by the fictional music streaming comany called Sparkify.

Data generated is of two types and are composed of set of JSON files. -
* **Songs Data -** Stored in the folder ./data/song_data. Static data about artists and songs within. 
* **Log Data -** Stored in the folder ./data/log_data. Simulated activity logs for the streaming app.

ETL solution is designed to perform the data ingestion.Technologies used - Python, SQL, PostgreSQL

---
## Sparkify Database

This section describes the database sparkifydb, designed in the form of a star schema. The database is of PostgreSQL type. 

_*SparkifyDB schema  ER Diagram.*_

### Fact Table

* **songplays**: song play data containing information about user, artist, and song info (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)

### Dimension Tables

* **users**: user info (columns: user_id, first_name, last_name, gender, level)
* **songs**: song info (columns: song_id, title, artist_id, year, duration)
* **artists**: artist info (columns: artist_id, name, location, latitude, longitude)
* **time**: detailed time info about song plays (columns: start_time, hour, day, week, month, year, weekday)

---

## How to use?

**Project has following files **

* **data**: Contains two sub-folders song_data and log_data. Under each sub-folders, the sample JSON files are present.
* **etl.ipynb**: Jupyter Notebook for interactive development using Python to see the intermediary results. Core logic used in this is fully implemented in etl.py file.
* **test.ipynb**: Jupyter Notebook for test the validity of SQL queries against the database.
* **create_tables.py**: This script drops tables if they exist and create new ones. Additionally it drops and creates the database as well.
* **etl.py**: This script uses data in ./data/song_data and ./data/log_data, processes it, and inserts the processed data into DB.
* **sql_queries.py**: Contains the core DDL logic for creation/dropping of table as well as the DML for data insertion.


### Prerequisites

Python3 is the recommended environment. Use Anaconda (https://www.anaconda.com/distribution/) either via GUI or command line for installing it.
Additionally, the following libraries are needed for the python environment to make Jupyter Notebook and Postgresql to work:

* _postgresql_ (+ dependencies) to enable sripts and Jupyter to connect to Postgresql DB.
* _jupyter_ (+ dependencies) to enable Jupyter Notebook.
* _ipython-sql_ (https://anaconda.org/conda-forge/ipython-sql) to make Jupyter Notebook and SQL queries to Postgresql work together. _(Note: you may need to install this library from command line)_.

### Run create_tables.py

Run the following command:

`python3 create_tables.py`

Expected Output: Script writes _"Tables dropped successfully"_ and _"Tables created successfully"_ if all tables were dropped and created without errors.
If the environment is properly set-up i.e. with correct PostgreSQL connection information, the syntactical errors are the only ones that may occur. In case of errors, check to ensure SQL are syntatically correct and try re-run.

### Run etl.py

Run the following command:

`python3 etl.py`

Output: Script reads through all the files in the log_data folder followed by song_data folder and inserts that data into the tables.:

* _"x files found in song_data/ "_ (e.g. data/song_data)
* _"x/x files processed "_ (e.g. data/log_data).


## Summary

Project-1 provides customer startup Sparkify tools to analyse their service data and help them answer their key business questions like _"What songs users are listening to"_.